{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Swarm Optimization (PSO) for ANN image recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import pandas as pd\n",
    "#from sklearn.datasets import load_files\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from ImageData import *\n",
    "from Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from Part 4, Linear Classifiers accuracy were very low for example SGD gave 44% accuracy \n",
    "## the interesting part for me is to check if PSO will improve accuracy on Images Dataset to some reasonable accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score on Testing Dataset = 0.42  with standard deviation = 0.02\n"
     ]
    }
   ],
   "source": [
    "### Load Data\n",
    "X,y = LoadData(type=\"Array\",use_PCA=\"False\")\n",
    "### Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100, shuffle=True)\n",
    "\n",
    "### Testing using SGD Classifier\n",
    "SGD_classifier = SGDClassifier(max_iter=1000, tol=1e-3 , shuffle=True , random_state=100)\n",
    "SGD_classifier.fit(X_train,y_train)\n",
    "\n",
    "scores = cross_val_score(SGD_classifier, X_test, y_test)\n",
    "print(\"Accuracy Score on Testing Dataset = %0.2f  with standard deviation = %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing functions for weights and Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import special\n",
    "import time\n",
    "from numpy import array\n",
    "from random import random\n",
    "from random import uniform\n",
    "from math import sin, sqrt\n",
    "from scipy.special import xlogy , xlog1py\n",
    "\n",
    "def Create_Weights_Bias_from_ParticleVector( weights, bias, pv):\n",
    "    i=0\n",
    "    for key,w in weights.items():\n",
    "        weights[key]=pv[i:i + w.shape[0]*w.shape[1]].reshape(w.shape)\n",
    "        i+=w.shape[0]*w.shape[1]\n",
    "\n",
    "    for key,b in bias.items():\n",
    "        bias[key]=pv[i:i + b.shape[0]*b.shape[1]].reshape(b.shape)\n",
    "        i+=b.shape[0]*b.shape[1]\n",
    "                       \n",
    "    return weights, bias\n",
    "\n",
    "def Create_Particles_Vector( weights, bias):\n",
    "    pv=[]\n",
    "    for key in weights:\n",
    "        for array in weights[key]:\n",
    "            for val in array:\n",
    "                pv.append(val)\n",
    "\n",
    "    for key in bias:\n",
    "        for array in bias[key]:\n",
    "            for val in array:\n",
    "                pv.append(val)\n",
    "    return pv\n",
    "\n",
    "\n",
    "\n",
    "def Activation_Function_funct(A , AType=\"Sigmoid, Relu, Tanh\"):\n",
    "    if AType == \"Sigmoid\":\n",
    "        a=np.zeros([1,1])\n",
    "        a=1/(1+np.exp(-A))\n",
    "        return a , a*(1-a) \n",
    "    elif AType == \"Tanh\":\n",
    "        t= (np.exp(A)-np.exp(-A))/(np.exp(A)+np.exp(-A)) #np.tanh(A) \n",
    "        dt=1-t**2\n",
    "        return t,dt\n",
    "    elif AType == \"Relu\":\n",
    "        R = np.maximum(0,A)\n",
    "        dR = np.where(R <= 0, 0, 1)\n",
    "        return  R , dR \n",
    "\n",
    "\n",
    "\n",
    "def cross_entropy_loss(Predicted, Actual):\n",
    "    _Loss = -1*(special.xlog1py(Actual,Predicted + 1e-15)+ special.xlog1py((1-Actual), (1-Predicted + 1e-15)))\n",
    "    #print(\"cross_entropy_loss shape\",_Loss.shape)\n",
    "    return np.mean(_Loss)\n",
    "\n",
    "\n",
    "def hinge_loss(Predicted, Actual):\n",
    "    new_predicted = np.array([-1 if i==0 else i for i in Predicted])\n",
    "    new_actual = np.array([-1 if i==0 else i for i in Actual])\n",
    "\n",
    "    hinge_loss = np.mean([max(0, 1-x*y) for x, y in zip(new_actual, new_predicted)])\n",
    "\n",
    "    print(\"hinge_loss shape\",hinge_loss.shape)\n",
    "    return hinge_loss\n",
    "\n",
    "def Loss_Fuction(Predicted, Actual, Loss=\"cross_entropy_loss , hinge_loss\"):\n",
    "    if Loss==\"cross_entropy_loss\":\n",
    "        return cross_entropy_loss(Predicted, Actual)\n",
    "    elif Loss==\"hinge_loss\":\n",
    "        return hinge_loss(Predicted, Actual)\n",
    "\n",
    "\n",
    "def forward_propagation(input,y, weights, bias, losstype,ActivationFunctions):\n",
    "    #print(\"New Forward Propagation code \")\n",
    "    A = {}\n",
    "    Z = {}\n",
    "    for i in range(0, weights.__len__()):\n",
    "        Z[i] = np.matmul(weights[i].T, input) + bias[i]\n",
    "        A[i],daaaa  = Activation_Function_funct(Z[i],ActivationFunctions[i])\n",
    "        input= A[i].copy()\n",
    "    #print(\"A = \", A)\n",
    "    W_dimention = weights.__len__() - 1\n",
    "    Last_Activation = A[W_dimention][0]#.copy()          ## Last activation function in our last layer   \n",
    "    #print(\"dimention \", W_dimention)\n",
    "    #print(\"Weights = \", weights)\n",
    "    #print(\"AA = \", Last_Activation)\n",
    "    #self.global_AA = Last_Activation.copy()\n",
    "    #print(\"Particle Predicted shape {}  = {}\".format(Last_Activation.shape, Last_Activation))\n",
    "    #print(\"Particle Actual shape {}  = {}\".format(y.shape, y))\n",
    "\n",
    "    Cost = Loss_Fuction(Last_Activation,y,losstype)\n",
    "    #print(\"J = \", J)\n",
    "    #print(y.shape[0])\n",
    "    ##Cost = J /y.shape[0]\n",
    "    #print(\"Cost = \", Cost)\n",
    "    return Cost \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Particle Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating PSO Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from random import random\n",
    "from random import uniform\n",
    "from math import sin, sqrt\n",
    "\n",
    "class PSO:\n",
    "    particles = []\n",
    "    gbest = 0\n",
    "    def Initialize_PSO(self, pop_size = 100,dimensions = 2):\n",
    "        #initialize the particles\n",
    "\n",
    "        for i in range(pop_size):\n",
    "            p = Particle()\n",
    "            p_v=array([uniform(-0.5,0.5) for i in range(dimensions)])\n",
    "            print(p_v)\n",
    "            p.params = p_v\n",
    "            p.fitness = 0.0\n",
    "            p.v = 0.0\n",
    "            p.best = p_v\n",
    "            p.informants = []\n",
    "            p.best_informant = 0\n",
    "            self.particles.append(p)\n",
    "        \n",
    "        self.gbest = self.particles[0]\n",
    "        \n",
    "\n",
    "    def addInformants(self, index, p):\n",
    "        from random import choice\n",
    "        pcount = self.particles.__len__()\n",
    "        n1=choice([i for i in range(0,pcount) if i not in [index]])\n",
    "        n2 = choice([i for i in range(0,pcount) if i not in [index,n1]])\n",
    "        n3 = choice([i for i in range(0,pcount) if i not in [index,n1,n2]])\n",
    "\n",
    "        p.informants.append(self.particles[n1].params)\n",
    "        p.informants.append(self.particles[n2].params)\n",
    "        p.informants.append(self.particles[n3].params)\n",
    "\n",
    "        return p.informants\n",
    "    \n",
    "    def bestInformant(self,p, input, y, weights, bias, losstype,ActivationFunctions):\n",
    "        best = float('inf')\n",
    "        ind=0\n",
    "        for inf in p.informants:\n",
    "            ind +=1\n",
    "            weights, bias = Create_Weights_Bias_from_ParticleVector(weights, bias, p.params)\n",
    "            fitness = forward_propagation(input,y, weights, bias, losstype,ActivationFunctions)\n",
    "\n",
    "            if fitness < best:\n",
    "                p.best_informant = p.params\n",
    "                best = fitness\n",
    "        return p.best_informant    \n",
    "                    \n",
    "\n",
    "    def Run_PSO(self, input, y, weights, bias, losstype, ActivationFunctions, iter_max ,c1 = 2,c2 = 2 , c3 =0.1 ,err_crit = 0.00001 ):\n",
    "        i=0\n",
    "        \n",
    "        # let the first particle be the global best\n",
    "        \n",
    "        err = 999999999\n",
    "        while i < iter_max :\n",
    "            for p in self.particles:\n",
    "                p.informants = []\n",
    "            \n",
    "            index=0\n",
    "            #print(\"itteration {} :\".format(i))\n",
    "            for p in self.particles:\n",
    "                #print(\"particle params : {} \".format(p.params))\n",
    "                p.informants = self.addInformants(index, p)\n",
    "                p.best_informant = self.bestInformant(p, input, y, weights, bias, losstype,ActivationFunctions)\n",
    "                #print(\"particle : {} , Best Informat : {}\".format(index,p.best_informant))\n",
    "                index +=1\n",
    "\n",
    "\n",
    "            for p in self.particles:\n",
    "                \n",
    "                _weights, _bias = Create_Weights_Bias_from_ParticleVector(weights, bias, p.params)\n",
    "\n",
    "                fitness = forward_propagation(input,y, _weights, _bias, losstype, ActivationFunctions) #f6(p.params)\n",
    "                #print('Cost for each particle {}:  {} err : {}'.format(i,fitness,err))\n",
    "                #print(\"fitness = \", fitness)\n",
    "                #print(\"p.fitness before = \", p.fitness)\n",
    "                if fitness < p.fitness:\n",
    "                    p.fitness = fitness\n",
    "                    #print(\"p.fitness After = \", p.fitness)\n",
    "                    p.best = p.params\n",
    "\n",
    "                if fitness < self.gbest.fitness:\n",
    "                    self.gbest = p\n",
    "                    #print(\"gbest.param = \", gbest.fitness)\n",
    "                v = p.v + c1 * random() * (p.best - p.params) \\\n",
    "                        + c2 * random() * (self.gbest.params - p.params) \\\n",
    "                        + c3 * random() * (p.best_informant - p.params)\n",
    "                p.params = p.params + v\n",
    "                \n",
    "            i  += 1\n",
    "            #if err < err_crit:\n",
    "            #    break\n",
    "            #progress bar. '.' = 10%\n",
    "            #if i % (iter_max/10) == 0:\n",
    "            #    print('.')\n",
    "\n",
    "        return self.gbest.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using CNN with PSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating CNN Class Code    https://github.com/feferna/psoCNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jad: Below code is not mine, writing my code will take very long time for this research questions. so i used code from the internet referenced below to see if PSO will work with CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://towardsdatascience.com/training-a-convolutional-neural-network-from-scratch-2235c2a25754"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Conv3D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST CNN initialized!\n",
      "--- Epoch 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jads\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Users\\jads\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Conv2D' object has no attribute 'forward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-04a19e2081c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m       \u001b[0mnum_correct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mnum_correct\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-04a19e2081c5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(im, label, lr)\u001b[0m\n\u001b[0;32m     50\u001b[0m   '''\n\u001b[0;32m     51\u001b[0m   \u001b[1;31m# Forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m   \u001b[1;31m# Calculate initial gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-04a19e2081c5>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(image, label)\u001b[0m\n\u001b[0;32m     31\u001b[0m   \u001b[1;31m# We transform the image from [0, 255] to [-0.5, 0.5] to make it easier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m   \u001b[1;31m# to work with. This is standard practice.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m   \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m   \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m   \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Conv2D' object has no attribute 'forward'"
     ]
    }
   ],
   "source": [
    "import mnist\n",
    "import numpy as np\n",
    "#from conv import Conv3x3\n",
    "#from maxpool import MaxPool2\n",
    "#from softmax import Softmax\n",
    "import keras\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "train_images = X_train.copy()\n",
    "train_labels = y_train.copy()\n",
    "test_images = X_test.copy()\n",
    "test_labels = y_test.copy()\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "conv = Conv2D(8,kernel_size=(3,3))                  # 28x28x1 -> 26x26x8\n",
    "pool = MaxPool2D()                  # 26x26x8 -> 13x13x8\n",
    "softmax = softmax(48 * 48 * 1) # 13x13x8 -> 10\n",
    "\n",
    "def forward(image, label):\n",
    "  '''\n",
    "  Completes a forward pass of the CNN and calculates the accuracy and\n",
    "  cross-entropy loss.\n",
    "  - image is a 2d numpy array\n",
    "  - label is a digit\n",
    "  '''\n",
    "  # We transform the image from [0, 255] to [-0.5, 0.5] to make it easier\n",
    "  # to work with. This is standard practice.\n",
    "  out = conv.forward((image / 255) - 0.5)\n",
    "  out = pool.forward(out)\n",
    "  out = softmax.forward(out)\n",
    "\n",
    "  # Calculate cross-entropy loss and accuracy. np.log() is the natural log.\n",
    "  loss = -np.log(out[label])\n",
    "  acc = 1 if np.argmax(out) == label else 0\n",
    "\n",
    "  return out, loss, acc\n",
    "\n",
    "def train(im, label, lr=.005):\n",
    "  '''\n",
    "  Completes a full training step on the given image and label.\n",
    "  Returns the cross-entropy loss and accuracy.\n",
    "  - image is a 2d numpy array\n",
    "  - label is a digit\n",
    "  - lr is the learning rate\n",
    "  '''\n",
    "  # Forward\n",
    "  out, loss, acc = forward(im, label)\n",
    "\n",
    "  # Calculate initial gradient\n",
    "  gradient = np.zeros(10)\n",
    "  gradient[label] = -1 / out[label]\n",
    "\n",
    "  # Backprop\n",
    "  gradient = softmax.backprop(gradient, lr)\n",
    "  gradient = pool.backprop(gradient)\n",
    "  gradient = conv.backprop(gradient, lr)\n",
    "\n",
    "  return loss, acc\n",
    "\n",
    "\n",
    "\n",
    "# Train the CNN for 3 epochs\n",
    "for epoch in range(3):\n",
    "  print('--- Epoch %d ---' % (epoch + 1))\n",
    "\n",
    "  # Shuffle the training data\n",
    "  permutation = np.random.permutation(len(train_images))\n",
    "  train_images = train_images[permutation]\n",
    "  train_labels = train_labels[permutation]\n",
    "\n",
    "  # Train!\n",
    "  loss = 0\n",
    "  num_correct = 0\n",
    "  for i, (im, label) in enumerate(zip(train_images, train_labels)):\n",
    "    if i > 0 and i % 100 == 99:\n",
    "      print(\n",
    "        '[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %\n",
    "        (i + 1, loss / 100, num_correct)\n",
    "      )\n",
    "      loss = 0\n",
    "      num_correct = 0\n",
    "\n",
    "    l, acc = train(im, label)\n",
    "    loss += l\n",
    "    num_correct += acc\n",
    "\n",
    "# Test the CNN\n",
    "print('\\n--- Testing the CNN ---')\n",
    "loss = 0\n",
    "num_correct = 0\n",
    "for im, label in zip(test_images, test_labels):\n",
    "  _, l, acc = forward(im, label)\n",
    "  loss += l\n",
    "  num_correct += acc\n",
    "\n",
    "num_tests = len(test_images)\n",
    "print('Test Loss:', loss / num_tests)\n",
    "print('Test Accuracy:', num_correct / num_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20ecc6adaa0cc8853378502dc8ee5c4a8aa07eb145d1b601d607be4cc35a2306"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('myenv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
